# !/usr/bin/env python
# -*-coding:utf-8 -*-
# @File    :   AccessLLM.py
# @Author  :   O_Orange 
# @Time    :   2025/02/25 09:26:53

import os
import fitz  # PyMuPDF
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.embeddings.base import Embeddings
from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_cors import CORS  # å…è®¸è·¨åŸŸè¯·æ±‚
from waitress import serve

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
load_dotenv()

OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
MODEL_NAME = os.getenv('MODEL_NAME')
CV_PATH = os.getenv("CV_PATH")

app = Flask(__name__)  # åˆå§‹åŒ–Flaskåº”ç”¨
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡ä¿å­˜é¢„å¤„ç†ç»“æœ
CV_CONTEXT, CV_VECTORSTORE = None, None

# é¢„åŠ è½½æ ‡å¿—
CV_PRELOADED = False


class M3EEmbedding(Embeddings):  # å®šä¹‰ M3EEmbedding ç±»
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        return self.model.encode(texts, normalize_embeddings=True).tolist()

    def embed_query(self, text):
        return self.model.encode([text], normalize_embeddings=True)[0].tolist()


def Parse_Embed_CV(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text += page.get_text("text")  # æå–çº¯æ–‡æœ¬

    # å°†æ–‡æœ¬åˆ†å—ä¸ºé€‚åˆ LLM å¤„ç†çš„ç‰‡æ®µ
    text_splitter = RecursiveCharacterTextSplitter(
        separators = '\n',
        chunk_size = 1000,  # æ¯å—å¤§å°
        chunk_overlap = 200  # å—é—´é‡å 
    )
    text_chunks = text_splitter.split_text(text)
    
    # ä½¿ç”¨ M3E-base ä½œä¸ºè¯åµŒå…¥æ¨¡å‹
    m3e_model = SentenceTransformer(model_name_or_path='D:/m3e-base', device="cuda")
    embedding_function = M3EEmbedding(m3e_model)
    vectorstore = FAISS.from_texts(text_chunks, embedding = embedding_function)
    return text, vectorstore


def Generate_Context(CV_text, CV_Embedding, JD):
    character_limit = 300  # å­—æ•°é™åˆ¶
    
    langchain_prompt_template = f"""
        ä½ å°†æ‰®æ¼”ä¸€ä½æ±‚èŒè€…çš„è§’è‰²,æ ¹æ®ä¸Šä¸‹æ–‡é‡Œçš„ç®€å†å†…å®¹ä»¥åŠåº”è˜å·¥ä½œçš„æè¿°,æ¥ç›´æ¥ç»™HRå†™ä¸€ä¸ªç¤¼è²Œä¸“ä¸š, ä¸”å­—æ•°ä¸¥æ ¼é™åˆ¶åœ¨{character_limit}ä»¥å†…çš„æ±‚èŒæ¶ˆæ¯,è¦æ±‚èƒ½å¤Ÿç”¨ä¸“ä¸šçš„è¯­è¨€ç»“åˆç®€å†ä¸­çš„ç»å†å’ŒæŠ€èƒ½,å¹¶ç»“åˆåº”è˜å·¥ä½œçš„æè¿°,æ¥é˜è¿°è‡ªå·±çš„ä¼˜åŠ¿,å°½æœ€å¤§å¯èƒ½æ‰“åŠ¨æ‹›è˜è€…ã€‚å§‹ç»ˆä½¿ç”¨ä¸­æ–‡æ¥è¿›è¡Œæ¶ˆæ¯çš„ç¼–å†™ã€‚å¼€å¤´æ˜¯æ‹›è˜è´Ÿè´£äºº, ç»“å°¾é™„ä¸Šæ±‚èŒè€…é‚®ç®±ã€‚è¿™æ˜¯ä¸€ä»½æ±‚èŒæ¶ˆæ¯ï¼Œä¸è¦åŒ…å«æ±‚èŒå†…å®¹ä»¥å¤–çš„ä¸œè¥¿,ä¾‹å¦‚â€œæ ¹æ®æ‚¨ä¸Šä¼ çš„æ±‚èŒè¦æ±‚å’Œä¸ªäººç®€å†,æˆ‘æ¥å¸®æ‚¨èµ·è‰ä¸€å°æ±‚èŒé‚®ä»¶ï¼šâ€è¿™ä¸€ç±»çš„å†…å®¹ï¼Œä¹Ÿä¸è¦æ­¤è‡´æ•¬ç¤¼ç­‰ç±»ä¼¼è¿‡äºæ­£å¼çš„è½æ¬¾, é‚®ç®±åé¢ä¸è¦æœ‰ä»»ä½•å†…å®¹! ä½¿ç”¨çº¯æ–‡æœ¬è¾“å‡ºï¼Œä¸è¦ç”¨Markdownæ ¼å¼è¾“å‡º,ï¼›æˆ‘ä¸æƒ³è¢«HRçœ‹å‡ºæ¥æ˜¯ä½¿ç”¨AIç”Ÿæˆçš„è¿™æ®µæ–‡å­—ã€‚
        å·¥ä½œæè¿°:
        {JD}"""+"""
        ç®€å†å†…å®¹:
        {context}
        è¦æ±‚:
        {question} 
    """
    question = "æ ¹æ®å·¥ä½œæè¿°ï¼Œå¯»æ‰¾å‡ºç®€å†é‡Œæœ€åˆé€‚çš„æŠ€èƒ½éƒ½æœ‰å“ªäº›?æ±‚èŒè€…çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ?"

    PROMPT = PromptTemplate.from_template(langchain_prompt_template)
    
    llm = ChatOpenAI(  # é€šè¿‡ API è°ƒç”¨ LLM å¤§æ¨¡å‹
        openai_api_base = OPENAI_API_BASE,
        openai_api_key= OPENAI_API_KEY,
        model = MODEL_NAME
    )

    # llm = Ollama(base_url='http://localhost:11434', model="deepseek-r1:1.5b")  # é€šè¿‡ Ollama è°ƒç”¨æœ¬åœ° LLM å¤§æ¨¡å‹

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=CV_Embedding.as_retriever(),
        chain_type="stuff",
        chain_type_kwargs={"prompt": PROMPT}
    )

    result = qa_chain.invoke({"query": question})
    letter = result['result']
    letter = letter.replace('\n', ' ')  # å»æ‰æ‰€æœ‰æ¢è¡Œç¬¦ï¼Œé˜²æ­¢åˆ†æˆå¤šæ®µæ¶ˆæ¯
    return letter


def initialize_rag_system():
    """åŒæ­¥åˆå§‹åŒ–å‡½æ•°"""
    global CV_CONTEXT, CV_VECTORSTORE, CV_PRELOADED
    if CV_PRELOADED:
        return
    
    try:
        if not os.path.exists(CV_PATH):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç®€å†æ–‡ä»¶: {CV_PATH}")
        
        CV_CONTEXT, CV_VECTORSTORE = Parse_Embed_CV(CV_PATH)
        CV_PRELOADED = True
        print(f"âœ… ç®€å†é¢„å¤„ç†å®Œæˆ: {CV_PATH}")
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise


@app.route('/generate', methods=['POST'])
def handle_generation():
    try:
        if not CV_PRELOADED:
            return jsonify({'error': 'ç®€å†æœªåŠ è½½'}), 503
            
        jd_data = request.get_json()
        if not jd_data or 'jd' not in jd_data:
            return jsonify({'error': 'éœ€è¦æä¾›å²—ä½æè¿°(jd)'}), 400
            
        letter = Generate_Context(CV_CONTEXT, CV_VECTORSTORE, jd_data['jd'])
        return jsonify({'result': letter.strip()}), 200
    
    except Exception as e:
        return jsonify({'error': 'ç”Ÿæˆå¤±è´¥: ' + str(e)}), 500


if __name__ == '__main__':
    # ç¡®ä¿å³ä½¿åœ¨éä¸»çº¿ç¨‹ç¯å¢ƒä¸‹ä¹Ÿä¼šåŠ è½½
    if not CV_PRELOADED:
        initialize_rag_system()
    
    print(f"ğŸš€ æœåŠ¡å·²å¯åŠ¨: http://localhost:5000/generate")
    app.run(host='0.0.0.0', port=5000)

    # JD = """
    #     ä¸€ã€å·¥ä½œå†…å®¹
    #     1ã€æœºæ¢°è®¾å¤‡æ™ºèƒ½åŒ–ã€è‡ªåŠ¨åŒ–æ–¹é¢é¡¹ç›®éœ€æ±‚åˆ†æã€ç³»ç»Ÿè®¾è®¡åŠæ ¸å¿ƒä»£ç çš„ç¼–å†™ï¼Œè§£å†³é¡¹ç›®å¼€å‘è¿‡ç¨‹ä¸­çš„æŠ€æœ¯é—®é¢˜ï¼›
    #     2.ç®—æ³•è®¾è®¡ï¼šé’ˆå¯¹å…·ä½“åº”ç”¨åœºæ™¯ï¼Œè®¾è®¡ç®—æ³•å¹¶æå‡å…¶æ•ˆæœ/æ•ˆç‡/ç¨³å®šæ€§
    #     äºŒã€ä»»èŒæ¡ä»¶ï¼š
    #     1.è®¡ç®—æœº/è‡ªåŠ¨åŒ–/ç”µå­ä¿¡æ¯/æ•°å­¦/ç­‰å·¥ç§‘ç›¸å…³ä¸“ä¸šï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼Œæœ‰2å¹´ä»¥ä¸Šè½¯ä»¶å¼€å‘ç»éªŒï¼›
    #     2.ç†Ÿç»ƒæŒæ¡C/C++/Matlab/Python/GoLangè‡³å°‘ä¸€ç§ï¼ŒåŠ¨æ‰‹èƒ½åŠ›å¼ºï¼›
    #     3.ç†Ÿæ‚‰å¸¸ç”¨CVåº“å’Œæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆCaffe/PyTorch/TensorFlowç­‰ï¼‰ï¼›
    # """  # ç”¨äºæµ‹è¯•
    # CV_context, CV_Embedding = Parse_Embed_CV(CV_PATH)
    # Letter = Generate_Context(CV_context, CV_Embedding, JD)
    # print(Letter)
